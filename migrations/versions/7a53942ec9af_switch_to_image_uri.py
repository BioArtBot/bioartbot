"""switch to image_uri

Revision ID: 7a53942ec9af
Revises: 5232f48e49c1
Create Date: 2021-10-24 12:26:55.765902

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

import io
import os
import math
import boto3
from botocore.exceptions import ClientError
from collections import namedtuple
from PIL import Image, ImageDraw

from migrations.utils.session import session_scope

# revision identifiers, used by Alembic.
revision = '7a53942ec9af'
down_revision = '5232f48e49c1'
branch_labels = None
depends_on = None

"""
ASSUMPTIONS BAKED INTO THIS DATA MIGRATION:
All previous canvas sizes were 39X26
All previous color mappings were {1:pink, 2:blue, 3:teal, 4:peach, 5:GFP}
An image bucket exists to store images in and its location is provided in env vars
NOTE: When downgrading, any artpiece larger than 39X26 will throw an error
    But if it's smaller, it will just add extra whitespace.
"""

AWS_SERVER = os.environ.get('AWS_SERVER')
AWS_PORT = int(os.environ.get('AWS_PORT', 4566))
AWS_ACCESS_KEY_ID = os.environ.get('AWS_ACCESS_KEY_ID')
AWS_SECRET_ACCESS_KEY = os.environ.get('AWS_SECRET_ACCESS_KEY')
AWS_DEFAULT_REGION = os.environ.get('AWS_DEFAULT_REGION')
IMAGE_BUCKET = os.environ.get('IMAGE_BUCKET')

RGBA = namedtuple('RGBA', ['r','g','b','a'])
color_mapping = {
    '1': RGBA(255, 192, 203, 1),
    '2': RGBA(0, 0, 255, 1),
    '3': RGBA(0, 128, 128, 1),
    '4': RGBA(255, 165, 0, 1),
    '5': RGBA(255, 255, 0, 1),
    }

class file_manager():
    def __init__(self, bucket=IMAGE_BUCKET, server= AWS_SERVER,
                       port=AWS_PORT, region=AWS_DEFAULT_REGION
                       ):
        self.endpoint_url = f'http://{server}:{port}' if server else None
        self.bucket = bucket
        try:
            self.s3 = boto3.client('s3', endpoint_url=self.endpoint_url, region_name=region)
        except ClientError as e:
            raise ClientError("""Failure connecting to S3 Media Storage.
                This migration script expects you to set the following environment vars:
                AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, IMAGE_BUCKET.
                AWS_SERVER, AWS_PORT must also be set if not using default Amazon endpoint.
                \n""" + e)

    @classmethod
    def parse_uri(cls, uri:str):
        try:
            uri = uri.split('://')[1]
            bucket = uri.split('/')[0]
            key = '/'.join(uri.split('/')[1:])
        except:
            return 'Malformed URI'
        return (bucket, key)

    def store_file(self, file, key):
        response = self.s3.upload_fileobj(file, self.bucket, key)
        return '/'.join([self.endpoint_url, self.bucket, key])

    def del_file(self, key):
        self.s3.delete_object(Bucket=self.bucket, Key=key)
        return True

fm = file_manager()

def make_file_from_bytes(bytes):
    image = Image.frombytes('RGBX', (616,414), bytes)
    with io.BytesIO() as output:
        image.save(output, format='JPEG')
        image_file = output.getvalue()
    return image_file

#make image bytes from JSON encoding
def decode_to_image(pixel_art_color_encoding, color_mapping
    , canvas_size=(39,26), scale=200):
    ratio = (3, 2)
    pixel_size = (ratio[0] * scale / canvas_size[0]
                 , ratio[1] * scale / canvas_size[1])
    total_size = (math.ceil(ratio[0] * scale + pixel_size[0])
                 , math.ceil(ratio[1] * scale + pixel_size[1]))
    im = Image.new('RGBX',total_size,(255,255,255,1))
    draw = ImageDraw.Draw(im)
    for color in pixel_art_color_encoding:
        # pixels are given as [y,x]
        for pixel_y, pixel_x in pixel_art_color_encoding[color]:
            origin = (pixel_size[0] * pixel_x, pixel_size[1] * pixel_y)
            far_corner = (pixel_size[0] + origin[0], pixel_size[1] + origin[1])
            draw.rectangle([origin, far_corner], fill=color_mapping[color])
    return (im.tobytes())


class ArtpieceModel(Base):
    __tablename__ = 'artpieces'

    id = sa.Column(sa.Integer, primary_key=True)
    slug = sa.Column(sa.String(60), nullable=False, unique=True, index=True)
    submit_date = sa.Column(sa.DateTime(), nullable=False)
    art = sa.Column(sa.JSON(), nullable=False, name='art_encoding')
    raw_image = sa.Column(sa.LargeBinary(), nullable=False)
    image_uri = sa.Column(sa.String(128), nullable=False)



def upgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('artpieces', sa.Column('image_uri', sa.String(length=128), nullable=True))
    
    with session_scope() as session:
        artpieces = session.query(ArtpieceModel).all()
        for artpiece in artpieces:
            image_file = make_file_from_bytes(artpiece.raw_image)
            artpiece.image_uri = fm.store_file(
                io.BytesIO(image_file),
                f'{artpiece.slug}_{int(artpiece.submit_date.timestamp()*1000)}.jpg'
                )

    op.alter_column('artpieces', 'image_uri', nullable=False)
    op.drop_column('artpieces', 'raw_image')

def downgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('artpieces', sa.Column('raw_image', postgresql.BYTEA(), autoincrement=False, nullable=True))


    with session_scope() as session:
        artpieces = session.query(ArtpieceModel).all()
        for artpiece in artpieces:
            artpiece.raw_image = decode_to_image(artpiece.art, color_mapping)
            fm.del_file(fm.parse_uri(artpiece.image_uri)[-1])
    
    op.alter_column('artpieces', 'raw_image', nullable=False)
    op.drop_column('artpieces', 'image_uri')
